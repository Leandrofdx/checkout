#!/bin/bash

echo "ğŸš€ Iniciando setup da IA Local..."

# Verificar se Docker estÃ¡ rodando
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker nÃ£o estÃ¡ rodando. Por favor, inicie o Docker Desktop."
    exit 1
fi

# Parar containers existentes
echo "ğŸ›‘ Parando containers existentes..."
docker-compose down

# Iniciar Ollama
echo "ğŸ³ Iniciando Ollama..."
docker-compose up -d

# Aguardar Ollama estar pronto
echo "â³ Aguardando Ollama estar pronto..."
sleep 10

# Verificar se Ollama estÃ¡ rodando
if curl -f http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama estÃ¡ rodando!"
else
    echo "âŒ Ollama nÃ£o estÃ¡ respondendo. Aguardando mais tempo..."
    sleep 20
fi

# Baixar modelo Llama 3.1 (8B)
echo "ğŸ“¥ Baixando modelo Llama 3.1 (8B)..."
curl -X POST http://localhost:11434/api/pull -d '{"name": "llama3.1:8b"}'

# Verificar modelos disponÃ­veis
echo "ğŸ“‹ Modelos disponÃ­veis:"
curl -s http://localhost:11434/api/tags | jq '.models[] | .name' 2>/dev/null || echo "Modelo ainda baixando..."

echo "ğŸ‰ Setup concluÃ­do! IA Local estÃ¡ pronta."
echo "ğŸŒ API disponÃ­vel em: http://localhost:11434"
echo "ğŸ“š DocumentaÃ§Ã£o: https://ollama.ai/library/llama3.1" 